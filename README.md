# Flux-AI Mobile (Odysseus)

This is the **Mobile** branch of Flux-AI. It contains the standalone web application designed for in-browser inference.

> **Looking for the Desktop version?**
> Switch to the `Desktop` branch to access the local inference client (Colossus).

## ðŸŒŸ Features

### ðŸ“± Mobile Client (Odysseus)

The Mobile client is a standalone web application that brings powerful AI to your phone without needing a backend server.

- **In-Browser AI:** Powered by **WebLLM**, it runs `Llama-3.2-3B` directly in your browser using WebGPU.
- **Thinking Mode:** Enables Chain-of-Thought reasoning for complex problem solving.
- **Zero Setup:** No server required â€“ just open the page and chat.

## ðŸš€ Getting Started

1. Open `Mobile/index.html` in a modern web browser (Chrome, Edge, or Arc recommended for WebGPU support).
2. **First Run:** Allow the application to download the model weights (approx. 2-3GB). This is stored locally in your browser cache.
3. Once initialized, you can chat offline.
